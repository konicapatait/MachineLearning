{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Means Clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    distance = math.sqrt((point1[0]-point2[0])**2 + (point1[1]-point2[1])**2)\n",
    "    return distance\n",
    "\n",
    "def assign_clusters(data, centroids):\n",
    "    clusters = [[] for _ in range(len(centroids))]\n",
    "    print(clusters)\n",
    "    for point in data:\n",
    "        distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
    "        print('distances: '+str(distances))\n",
    "        closest_centroid = distances.index(min(distances))\n",
    "        print('closest_centroid: '+str(closest_centroid))\n",
    "        clusters[closest_centroid].append(point)\n",
    "    return clusters\n",
    "\n",
    "def update_centroids(clusters):\n",
    "    centroids = []\n",
    "    for cluster in clusters:\n",
    "        if cluster:\n",
    "            x_values = [point[0] for point in cluster]\n",
    "            y_values = [point[1] for point in cluster]\n",
    "            centroid_x = sum(x_values)/len(cluster)\n",
    "            centroid_y = sum(y_values)/len(cluster)\n",
    "            centroid = [centroid_x, centroid_y]\n",
    "            centroids.append(centroid)\n",
    "    return centroids\n",
    "\n",
    "\n",
    "\n",
    "def k_means_clustering(centroids, dataset):\n",
    "\n",
    "#   Description: Perform k means clustering for 2 iterations given as input the dataset and centroids.\n",
    "#   Input:\n",
    "#       1. centroids - A list of lists containing the initial centroids for each cluster. \n",
    "#       2. dataset - A list of lists denoting points in the space.\n",
    "#   Output:\n",
    "#       1. results - A dictionary where the key is iteration number and store the cluster assignments in the \n",
    "#           appropriate clusters. Also, update the centroids list after each iteration.\n",
    "\n",
    "    result = {\n",
    "        '1': { 'cluster1': [], 'cluster2': [], 'cluster3': [], 'centroids': []},\n",
    "        '2': { 'cluster1': [], 'cluster2': [], 'cluster3': [], 'centroids': []}\n",
    "    }\n",
    "    \n",
    "    centroid1, centroid2, centroid3 = centroids[0], centroids[1], centroids[2]\n",
    "    \n",
    "    for iteration in range(2):\n",
    "        # your code here\n",
    "        \n",
    "        clusters = assign_clusters(dataset, centroids)\n",
    "        print('Clusters:: '+str(clusters))\n",
    "        centroids = update_centroids(clusters)\n",
    "        print('centroids:: '+str(centroids))\n",
    "        result[str(iteration+1)]['centroids'] = centroids\n",
    "        for i in range(len(clusters)):\n",
    "            result[str(iteration+1)]['cluster'+str(i+1)] = clusters[i]\n",
    "\n",
    "        print('result:: '+str(result))\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans Data: [[46, 33], [26, 21], [23, 96], [82, 20], [25, 42], [29, 99], [30, 64], [57, 51], [12, 68], [25, 9]]\n",
      "Centroid Data: [[12, 68], [46, 33], [25, 42]]\n",
      "[[], [], []]\n",
      "distances: [48.79549159502341, 0.0, 22.847319317591726]\n",
      "closest_centroid: 1\n",
      "distances: [49.040799340956916, 23.323807579381203, 21.02379604162864]\n",
      "closest_centroid: 2\n",
      "distances: [30.083217912982647, 67.06713054842886, 54.037024344425184]\n",
      "closest_centroid: 0\n",
      "distances: [84.87638069569178, 38.27531841800928, 61.09828148156051]\n",
      "closest_centroid: 1\n",
      "distances: [29.068883707497267, 22.847319317591726, 0.0]\n",
      "closest_centroid: 2\n",
      "distances: [35.35533905932738, 68.15423684555495, 57.14017850864661]\n",
      "closest_centroid: 0\n",
      "distances: [18.439088914585774, 34.88552708502482, 22.561028345356956]\n",
      "closest_centroid: 0\n",
      "distances: [48.104053883222775, 21.095023109728988, 33.24154027718932]\n",
      "closest_centroid: 1\n",
      "distances: [0.0, 48.79549159502341, 29.068883707497267]\n",
      "closest_centroid: 0\n",
      "distances: [60.41522986797286, 31.89043743820395, 33.0]\n",
      "closest_centroid: 1\n",
      "Clusters:: [[[23, 96], [29, 99], [30, 64], [12, 68]], [[46, 33], [82, 20], [57, 51], [25, 9]], [[26, 21], [25, 42]]]\n",
      "centroids:: [[23.5, 81.75], [52.5, 28.25], [25.5, 31.5]]\n",
      "result:: {'1': {'cluster1': [[23, 96], [29, 99], [30, 64], [12, 68]], 'cluster2': [[46, 33], [82, 20], [57, 51], [25, 9]], 'cluster3': [[26, 21], [25, 42]], 'centroids': [[23.5, 81.75], [52.5, 28.25], [25.5, 31.5]]}, '2': {'cluster1': [], 'cluster2': [], 'cluster3': [], 'centroids': []}}\n",
      "[[], [], []]\n",
      "distances: [53.69182898728632, 8.050621094052309, 20.554804791094465]\n",
      "closest_centroid: 1\n",
      "distances: [60.80141856897748, 27.473851204372494, 10.51189802081432]\n",
      "closest_centroid: 2\n",
      "distances: [14.2587692315992, 73.89392735536528, 64.54843142943135]\n",
      "closest_centroid: 0\n",
      "distances: [85.0606401339656, 30.631886980726474, 57.65847726050351]\n",
      "closest_centroid: 1\n",
      "distances: [39.77829181852836, 30.745934690622107, 10.51189802081432]\n",
      "closest_centroid: 2\n",
      "distances: [18.105593058499906, 74.55073775624223, 67.59067983087608]\n",
      "closest_centroid: 0\n",
      "distances: [18.90271144571593, 42.24112332786618, 32.810059433045836]\n",
      "closest_centroid: 0\n",
      "distances: [45.473206396734334, 23.19078480776362, 37.04726710568541]\n",
      "closest_centroid: 1\n",
      "distances: [17.92519177024335, 56.747797314080834, 38.91657744458009]\n",
      "closest_centroid: 0\n",
      "distances: [72.76546227435101, 33.56802794326768, 22.5055548698538]\n",
      "closest_centroid: 2\n",
      "Clusters:: [[[23, 96], [29, 99], [30, 64], [12, 68]], [[46, 33], [82, 20], [57, 51]], [[26, 21], [25, 42], [25, 9]]]\n",
      "centroids:: [[23.5, 81.75], [61.666666666666664, 34.666666666666664], [25.333333333333332, 24.0]]\n",
      "result:: {'1': {'cluster1': [[23, 96], [29, 99], [30, 64], [12, 68]], 'cluster2': [[46, 33], [82, 20], [57, 51], [25, 9]], 'cluster3': [[26, 21], [25, 42]], 'centroids': [[23.5, 81.75], [52.5, 28.25], [25.5, 31.5]]}, '2': {'cluster1': [[23, 96], [29, 99], [30, 64], [12, 68]], 'cluster2': [[46, 33], [82, 20], [57, 51]], 'cluster3': [[26, 21], [25, 42], [25, 9]], 'centroids': [[23.5, 81.75], [61.666666666666664, 34.666666666666664], [25.333333333333332, 24.0]]}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1': {'cluster1': [[23, 96], [29, 99], [30, 64], [12, 68]],\n",
       "  'cluster2': [[46, 33], [82, 20], [57, 51], [25, 9]],\n",
       "  'cluster3': [[26, 21], [25, 42]],\n",
       "  'centroids': [[23.5, 81.75], [52.5, 28.25], [25.5, 31.5]]},\n",
       " '2': {'cluster1': [[23, 96], [29, 99], [30, 64], [12, 68]],\n",
       "  'cluster2': [[46, 33], [82, 20], [57, 51]],\n",
       "  'cluster3': [[26, 21], [25, 42], [25, 9]],\n",
       "  'centroids': [[23.5, 81.75],\n",
       "   [61.666666666666664, 34.666666666666664],\n",
       "   [25.333333333333332, 24.0]]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open ('data/sample_dataset_kmeans.pickle', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "print('Kmeans Data: '+str(dataset))\n",
    "with open ('./data/sample_centroids_kmeans.pickle', 'rb') as f:\n",
    "    centroids = pickle.load(f)\n",
    "print('Centroid Data: '+str(centroids))\n",
    "\n",
    "k_means_clustering(centroids,dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM Clustering\n",
    "EM (Expectation-Maximization) algorithm for clustering is an iterative method used to estimate the parameters of a Gaussian mixture model. Gaussian mixture model assumes that the data is generated from a mixture of several Gaussian distributions with unknown parameters. EM algorithm tries to learn these parameters from the data. The algorithm alternates between two steps: the E-step and the M-step.\n",
    "\n",
    "In the E-step, the algorithm computes the posterior probabilities that each data point belongs to each of the Gaussian components given the current estimate of the parameters. This is done using Bayes' rule and the current estimate of the parameters. These probabilities are called the \"responsibilities\" and are used to assign each data point to one of the Gaussian components.\n",
    "\n",
    "In the M-step, the algorithm computes a new estimate of the parameters of each Gaussian component given the responsibilities of the data points. This is done using maximum likelihood estimation by treating the responsibilities as weights for the data points and computing the mean and variance of the data points in each Gaussian component.\n",
    "\n",
    "The E-step and M-step are repeated until the algorithm converges, i.e., until the change in the log-likelihood of the data is smaller than a threshold or the maximum number of iterations is reached. At convergence, the algorithm outputs the estimates of the parameters for each Gaussian component, which can be used for clustering the data.\n",
    "\n",
    "In summary, EM algorithm for clustering is a way of fitting a Gaussian mixture model to the data by iteratively estimating the parameters of the model using the E-step and M-step. The algorithm is widely used in machine learning and data analysis because it is simple, flexible, and can be applied to a wide range of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "Let x be the value of a data point, \n",
    "let u and v be the mean and standard deviation of a Gaussian distribution, \n",
    "then the probability of x belonging to the Gaussian distribution is computed as \n",
    "\n",
    "```f(x, u, v) = (1 / (v * sqrt(2 * 3.14))) * e^(-1/2 * ((x - u)/v)^2)```\n",
    "\n",
    "Given 10 data points: [15, 6, 3, 2, 10, 16, 3, 5, 11, 10]\n",
    "and two initial clusters: [[2, 7], [12, 2]], \n",
    "where [2, 7] are the mean and standard deviation of the first cluster. \n",
    "\n",
    "**E-step:** Compute the probability of each data point belonging to each cluster \n",
    "\n",
    "for each x, for each cluster (u, v), compute f(x, u, v): \n",
    "e.g., x = 15, cluster 1 (u = 2, v = 7), cluster 2 (u = 12, v = 2) \n",
    "probabilities p1 = f(15, 2, 7), p2 = f(15, 12, 2) \n",
    "\n",
    "normalize the probabilities: \n",
    "e.g., p1' = p1 / (p1 + p2), p2' = p2 / (p1 + p2) \n",
    "\n",
    "**M-step:** Update the clusters (i.e., the u and v values) based on the probabilities \n",
    "of x belonging to each cluster \n",
    "\n",
    "u1' = weighted sum of x belonging to the first cluster \n",
    "e.g., 0.3 probability for 15 (x1), 0.5 probability for 6 (x2), ... \n",
    "u1' = (0.3 * 15 + 0.5 * 6 + ...) / (0.3 + 0.5 + ... ) \n",
    "v1' = updated standard deviation \n",
    "v1' = sqrt((0.3 * (15 - u1')^2 + 0.5 * (6 - u1')^2 + ...) / (0.3 + 0.5 + ...)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def em_clustering(centroids, dataset):\n",
    "\n",
    "#   Input: \n",
    "#       1. centroids - A list of lists with each value representing the mean and standard deviation values picked from a gausian distribution.\n",
    "#       2. dataset - A list of points randomly picked.\n",
    "#   Output:\n",
    "#       1. results - Return the updated centroids(updated mean and std values after the EM step) after the first iteration.\n",
    "\n",
    "    new_centroids = list()\n",
    "    \n",
    "    # your code here\n",
    "    dataset = np.array(dataset)\n",
    "    print('dataset: '+str(dataset))\n",
    "    means = np.array(centroids)[:, 0]\n",
    "    stds = np.array(centroids)[:, 1]\n",
    "    print('means: '+str(means) + ', stds: '+str(stds))\n",
    "    # initialize the responsibilities for each data point\n",
    "    resps = np.zeros((len(dataset), len(centroids)))\n",
    "    print('resps: '+str(resps) )\n",
    "    # perform EM iterations\n",
    "    max_iter = 1\n",
    "    eps = 1e-6\n",
    "    for i in range(max_iter):\n",
    "        # E-step: compute the responsibilities for each data point\n",
    "        for j in range(len(dataset)):\n",
    "            p = np.exp(-(dataset[j] - means)**2 / (2 * stds**2)) / np.sqrt(2 * np.pi * stds**2)\n",
    "            print('Individial Probabilities: '+str(p) + ', np.sum(p):'+str(np.sum(p) ))\n",
    "            resps[j] = p / np.sum(p)\n",
    "        \n",
    "        # M-step: compute the updated means and stds for each Gaussian component\n",
    "        means_new = np.sum(resps * dataset[:, np.newaxis], axis=0) / np.sum(resps, axis=0)\n",
    "        stds_new = np.sqrt(np.sum(resps * (dataset[:, np.newaxis] - means_new)**2, axis=0) / np.sum(resps, axis=0))\n",
    "        \n",
    "        # check for convergence\n",
    "        if np.allclose(means, means_new, rtol=eps) and np.allclose(stds, stds_new, rtol=eps):\n",
    "            break\n",
    "        means, stds = means_new, stds_new\n",
    "    \n",
    "    # assign each data point to the closest Gaussian component\n",
    "    labels = np.argmax(resps, axis=1)\n",
    "    \n",
    "    # return the updated centroids\n",
    "    new_centroids = [[means[i], stds[i]] for i in range(len(centroids))]\n",
    "    \n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans Data: [8, 16, 13, 9, 18, 15, 7, 5, 7, 3]\n",
      "Centroid Data: [[13.346550530668159, 3.236599802533008], [7.9971108077796735, 4.473417525043109]]\n",
      "dataset: [ 8 16 13  9 18 15  7  5  7  3]\n",
      "means: [13.34655053  7.99711081], stds: [3.2365998  4.47341753]\n",
      "resps: [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "p: [0.03149727 0.08918063], np.sum(p):0.12067789946975242\n",
      "p: [0.08807915 0.01800097], np.sum(p):0.10608011978189746\n",
      "p: [0.12255515 0.04771759], np.sum(p):0.17027274112584723\n",
      "p: [0.05002651 0.08696744], np.sum(p):0.136993955967132\n",
      "p: [0.04384753 0.00732031], np.sum(p):0.05116783774094648\n",
      "p: [0.10818086 0.02618941], np.sum(p):0.13437026398083202\n",
      "p: [0.01802552 0.08699256], np.sum(p):0.10501807531479984\n",
      "p: [0.00443347 0.07125221], np.sum(p):0.07568568739205096\n",
      "p: [0.01802552 0.08699256], np.sum(p):0.10501807531479984\n",
      "p: [0.00074434 0.04778653], np.sum(p):0.0485308667298383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[13.690496166263058, 3.7579433264150217],\n",
       " [7.440207823033494, 3.6206084779437697]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open ('./data/sample_dataset_em.pickle', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "print('Kmeans Data: '+str(dataset))\n",
    "with open ('./data/sample_result_em.pickle', 'rb') as f:\n",
    "    centroids = pickle.load(f)\n",
    "print('Centroid Data: '+str(centroids))\n",
    "\n",
    "em_clustering(centroids,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
